{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Means Clustering and Choosing K\n",
    "## (and some other challenges)\n",
    "\n",
    " \n",
    "\n",
    "_Again, this is all available at [ajbc.io/MLintro](ajbc.io/MLintro)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from plotnine import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate some simulated data!\n",
    "(Based on [this demo](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "# create some data\n",
    "obs, true_clusters = make_blobs(n_samples=1000, centers=6, random_state=42)\n",
    "data = pd.DataFrame(obs, columns=['D1', 'D2'])\n",
    "data['true_cluster'] = true_clusters \n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "(ggplot(data)\n",
    " + aes(x='D1', y='D2', color='factor(true_cluster)')\n",
    " + geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit K-means with K=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(obs)\n",
    "data['learned_cluster'] = kmeans.labels_\n",
    "\n",
    "(ggplot(data)\n",
    " + aes(x='D1', y='D2', color='factor(learned_cluster)')\n",
    " + geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# How do we choose \"K\"?  One approach: the \"elbow\" method\n",
    "sse_log = list()\n",
    "for k in range(1,11):\n",
    "    print(\"Fitting K =\", k)\n",
    "    model = KMeans(n_clusters=k, random_state=0).fit(obs)\n",
    "    print(\"\\tSSS =\", model.inertia_)\n",
    "    sse_log.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sse_data = pd.DataFrame({\"K\": range(1,11), \"SSE\": sse_log})\n",
    "(ggplot(sse_data)\n",
    " + aes(x='K', y='SSE')\n",
    " + geom_point() + geom_line()\n",
    " + ylab(\"sum of squared errors\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(obs)\n",
    "data['learned_cluster'] = kmeans.labels_\n",
    "\n",
    "(ggplot(data)\n",
    " + aes(x='D1', y='D2', color='factor(learned_cluster)')\n",
    " + geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing K for other models\n",
    "- Likelihood\n",
    "- Bayesian information criterion (BIC)\n",
    "- Perplexity\n",
    "- ...\n",
    "\n",
    "### More resources\n",
    "- [Gaussian Mixture Model clustering: how to select the number of components (clusters)](https://towardsdatascience.com/gaussian-mixture-model-clusterization-how-to-select-the-number-of-components-clusters-553bef45f6e4)\n",
    "- [Perplexity To Evaluate Topic Models](http://qpleple.com/perplexity-to-evaluate-topic-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What other issues might we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Anisotropicly distributed data\n",
    "transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "obs2 = np.dot(obs, transformation)\n",
    "data = pd.DataFrame(obs2, columns=['D1', 'D2'])\n",
    "data['learned_cluster'] = KMeans(n_clusters=4, random_state=42).fit(obs2).labels_\n",
    "\n",
    "(ggplot(data) + aes(x='D1', y='D2', color='factor(learned_cluster)') + geom_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Different variance\n",
    "obs, true_clusters = make_blobs(n_samples=1000, centers=3, \n",
    "                                cluster_std=[1.0, 2.5, 0.5], random_state=170)\n",
    "data = pd.DataFrame(obs, columns=['D1', 'D2'])\n",
    "data['learned_cluster'] = KMeans(n_clusters=3).fit(obs).labels_\n",
    "\n",
    "(ggplot(data) + aes(x='D1', y='D2', color='factor(learned_cluster)') + geom_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Dimension have very different ranges\n",
    "obs, true_clusters = make_blobs(n_samples=1000, centers=4, random_state=42)\n",
    "obs[:,1] *= 10\n",
    "data = pd.DataFrame(obs, columns=['D1', 'D2'])\n",
    "data['learned_cluster'] = KMeans(n_clusters=4).fit(obs).labels_\n",
    "\n",
    "(ggplot(data) + aes(x='D1', y='D2', color='factor(learned_cluster)') + geom_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# There may not actually be clusters!\n",
    "obs = np.random.random((1000, 2))\n",
    "data = pd.DataFrame(obs, columns=['D1', 'D2'])\n",
    "data['learned_cluster'] = KMeans(n_clusters=3).fit(obs).labels_\n",
    "\n",
    "(ggplot(data) + aes(x='D1', y='D2', color='factor(learned_cluster)') + geom_point())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
